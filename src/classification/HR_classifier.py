# -*- coding: utf-8 -*-
"""01-Udemy-DS4B-Human Resources Department

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HWMTnxG8PL9P-fkSTSevIzBKSL3gAHyO

# TASK #1: UNDERSTAND THE PROBLEM STATEMENT AND BUSINESS CASE

<table>
  <tr><td>
    <img src="https://drive.google.com/uc?id=1yJKgmHrRFnBk987HJfeDrMcTEXtk0z7W"
         alt="Fashion MNIST sprite"  width="1000">
  </td></tr>
  <tr><td align="center">
    <b>Figure 1. Employee Retention Prediction
  </td></tr>
</table>

![alt text](https://drive.google.com/uc?id=10NJUOTWOBzp2MNkgcPpCF0fLtdoN_jKj)

![alt text](https://drive.google.com/uc?id=1evbDHoW2t0emxkbQd8yevYFZ5woJKRPY)

![alt text](https://drive.google.com/uc?id=1Mk2H7VYfv6ijUS9XqEdBQV6_LaHiyvkJ)

# TASK #2: IMPORT LIBRARIES AND DATASETS
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# You will need to mount your drive using the following commands:
# For more information regarding mounting, please check this out: https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory

from google.colab import drive
drive.mount('/content/drive')

# You have to include the full link to the csv file containing your dataset
employee_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data Science for Business/1. Human Resources Data/Human_Resources.csv')

employee_df

employee_df.head()

employee_df.tail()

# 35 features in total, each contains 1470 data points

employee_df.info()

employee_df["Age"].mean()

employee_df["Age"].describe()

"""# TASK #3: VISUALIZE DATASET"""

# Let's replace 'Attritition' , 'overtime' , 'Over18' column with integers before performing any visualizations 
employee_df["Attrition"] = employee_df["Attrition"].apply(lambda x:1 if x == 'Yes' else 0)
# employee_df['OverTime'] = pd.get_dummies(employee_df["OverTime"])['Yes']
employee_df['OverTime'] = employee_df["OverTime"].apply(lambda y: 1 if y == 'Yes' else 0)
employee_df['Over18'] = employee_df['Over18'].apply(lambda age18: 1 if age18 == 'Y' else 0)

employee_df.head()

# employee_df["novo"] = [np.nan]*1470
# employee_df.describe()

# Let's see if we have any missing data, luckily we don't!
sns.heatmap(employee_df.isna(), cmap= 'Blues', cbar=False)

# Several features such as 'MonthlyIncome' and 'TotalWorkingYears' are tail heavy
# It makes sense to drop 'EmployeeCount' and 'Standardhours' since they do not change from one employee to the other
employee_df.hist(bins=30, figsize=(20, 20), color='r')

# It makes sense to drop 'EmployeeCount' , 'Standardhours' and 'Over18' since they do not change from one employee to the other
# Let's drop 'EmployeeNumber' as well

employee_df.drop(['EmployeeCount', 'StandardHours', 'Over18', 'EmployeeNumber'], axis=1, inplace=True) # axis=1 is column. KeyError: "['EmployeeCount' 'Standardhours' 'Over18' 'EmployeeNumber'] not found in axis"

# Let's see how many employees left the company! 
left_df = employee_df[employee_df['Attrition']==1]
stayed_df = employee_df[employee_df['Attrition']==0]

# Count the number of employees who stayed and left
# It seems that we are dealing with an imbalanced dataset 
print('Total employees = ', len(employee_df))
print('left = ', len(left_df))
print('left_pct = ', 1.*len(left_df)/len(employee_df)*100, '%')

print('right = ', len(stayed_df))
print('left_pct = ', 1.*len(stayed_df)/len(employee_df)*100, '%')

left_df.describe()

#  Let's compare the mean and std of the employees who stayed and left 
# 'age': mean age of the employees who stayed is higher compared to who left
# 'DailyRate': Rate of employees who stayed is higher
# 'DistanceFromHome': Employees who stayed live closer to home 
# 'EnvironmentSatisfaction' & 'JobSatisfaction': Employees who stayed are generally more satisifed with their jobs
# 'StockOptionLevel': Employees who stayed tend to have higher stock option level

stayed_df.describe()

correlations = employee_df.corr()
f, ax = plt.subplots(figsize=(30,20))
sns.heatmap(correlations, annot=True)

# Job level is strongly correlated with total working hours   0.78
# Monthly income is strongly correlated with Job level        0.95
# Monthly income is strongly correlated with total working hours  0.77
# Age is stongly correlated with monthly income                   0.5

plt.figure(figsize=[25, 15])
sns.countplot(x='Age', hue='Attrition', data=employee_df)

plt.figure(figsize=[27, 20])
plt.subplot(221)
g = sns.countplot(x='JobRole', hue='Attrition', data=employee_df)
# g.set_xticklabels(rotation=30)
plt.xticks(rotation=20)
plt.subplot(222)
sns.countplot(x='MaritalStatus', hue='Attrition', data=employee_df)

plt.subplot(223)
sns.countplot(x='JobInvolvement', hue='Attrition', data=employee_df)

plt.subplot(224)
sns.countplot(x='JobLevel', hue='Attrition', data=employee_df)

# Single employees tend to leave compared to married and divorced
# Sales Representitives tend to leave compared to any other job 
# Less involved employees tend to leave the company 
# Less experienced (low job level) tend to leave the company

# KDE (Kernel Density Estimate) is used for visualizing the Probability Density of a continuous variable. 
# KDE describes the probability density at different values in a continuous variable. 
plt.figure(figsize=(12,7))
sns.kdeplot(left_df['DistanceFromHome'], label='Employees who left', shade=True, color='r')#, legend=True, common_grid=True)
sns.kdeplot(stayed_df['DistanceFromHome'], label='Employees who stayed', shade=True, color='gray', legend=True)
plt.xlabel('Distance from Home')

column = 'YearsWithCurrManager'
plt.figure(figsize=(12,7))
sns.kdeplot(left_df[column], label='Employees who left', shade=True, color='r')#, legend=True, common_grid=True)
sns.kdeplot(stayed_df[column], label='Employees who stayed', shade=True, color='gray', legend=True)
plt.xlabel('Years With Current Manager')

column = 'TotalWorkingYears'
plt.figure(figsize=(12,7))
sns.kdeplot(left_df[column], label='Employees who left', shade=True, color='r')#, legend=True, common_grid=True)
sns.kdeplot(stayed_df[column], label='Employees who stayed', shade=True, color='gray', legend=True)
plt.xlabel('Total Working Years')

# Let's see the Gender vs. Monthly Income
plt.figure(figsize=(18,7))
# sns.boxplot(y=employee_df['Gender'], x=employee_df['MonthlyIncome'])
sns.boxplot(y='Gender', x='MonthlyIncome', data=employee_df)

# Let's see the Job Role vs. Monthly Income
  plt.figure(figsize=(15, 7))
  sns.boxplot(y='JobRole', x='MonthlyIncome', data=employee_df)

plt.figure(figsize=(25,8))
sns.boxplot(data=employee_df, y='Age', x='MonthlyIncome')

"""# TASK #4: CREATE TESTING AND TRAINING DATASET & PERFORM DATA CLEANING"""

employee_df.head()

X_cat = employee_df[['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']]

X_cat

from sklearn.preprocessing import OneHotEncoder
onehotencoder = OneHotEncoder()
X_cat = onehotencoder.fit_transform(X_cat).toarray()
X_cat

X_cat.shape

X_cat = pd.DataFrame(X_cat)
 X_cat

# note that we dropped the target 'Atrittion'
X_numeric = employee_df[['Age', 'DailyRate', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 
                         'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',
                         'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']]

X_all = pd.concat([X_cat, X_numeric], axis=1)

X_all

from sklearn.preprocessing import MinMaxScaler
scalar = MinMaxScaler()
X = scalar.fit_transform(X_all)
X

y = employee_df['Attrition']
y

"""# TASK #5: UNDERSTAND THE INTUITION BEHIND LOGISTIC REGRESSION CLASSIFIERS, ARTIFICIAL NEURAL NETWORKS, AND RANDOM FOREST CLASSIFIER

![alt text](https://drive.google.com/uc?id=19DpnhFkfsNEDPlH1dkfdr1zO36vRcBit)

![alt text](https://drive.google.com/uc?id=1J03xZf6OiYtGV3IgJBUURBWyScpvaAbU)

![alt text](https://drive.google.com/uc?id=1WNsznVn7je5r9HGnSLLdABICxrIv2Mrs)

![alt text](https://drive.google.com/uc?id=1bX5uGmy5vbYTlp7m4tw_V2kTNzAHpHLp)

![alt text](https://drive.google.com/uc?id=1ztrMNehNYWMw6NwhOOC9BDBdnoNirpqZ)

# TASK #6: UNDERSTAND HOW TO ASSESS CLASSIFICATION MODELS

![alt text](https://drive.google.com/uc?id=1OZLbKm1AJSyvoBgfvlfcLIWZxLOvzOWq)

![alt text](https://drive.google.com/uc?id=11pNdVw4oWeNOWrkadrrxon7FU4qO5m6U)

![alt text](https://drive.google.com/uc?id=1Bk1xFW2tGBdwg-njOhw79MxtYBQnK-6x)

![alt text](https://drive.google.com/uc?id=19cXoBqSiqbEGNofnD603bz3xEAsX28hy)

# TASK #7: TRAIN AND EVALUATE A LOGISTIC REGRESSION CLASSIFIER
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)

X_train.shape

X_test.shape

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# Testing Set Performance
y_pred

from sklearn.metrics import confusion_matrix, classification_report
print('Accuracy {} %'. format(100*accuracy_score(y_test, y_pred)))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm,annot=True)

print(classification_report(y_test, y_pred))

"""# TASK #8: TRAIN AND EVALUATE A RANDOM FOREST CLASSIFIER"""

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred

# Testing Set Performance
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, cbar=False)
print('Accuracy {} %'. format(100*accuracy_score(y_test, y_pred)))

print(classification_report(y_test, y_pred))

"""# TASK #9: TRAIN AND EVALUATE A DEEP LEARNING MODEL """

import tensorflow as tf
model = tf.keras.models.Sequential()

"""![alt text](https://images4.programmersought.com/824/e0/e05cadf14cfab7a6a130cb04e2ac9a10.png "Activation Functions")

input_shape is the dimension of each register
"""

print('X_train.shape = ', X_train.shape, 'so input_shape = ', X_train.shape[1])
model.add(tf.keras.layers.Dense(units=500, activation='relu', input_shape=(50,)))
model.add(tf.keras.layers.Dense(units=500, activation='relu'))
model.add(tf.keras.layers.Dense(units=500, activation='relu'))
model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
# ReLU stands for Rectified Linear Unit

model.summary()

model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])

epochs_hist = model.fit(X_train, y_train, epochs = 100, batch_size = 50)

# oversampler = SMOTE(random_state=0)
# smote_train, smote_target = oversampler.fit_sample(X_train, y_train)
# epochs_hist = model.fit(smote_train, smote_target, epochs = 100, batch_size = 50)
y_pred = model.predict(X_test)
#y_pred = np.array(y_pred<0.5)
y_pred = y_pred>0.5
y_pred = list(map(int, y_pred))
# y_pred = list(map(lambda x: int(x), y_pred))
y_pred

plt.plot(epochs_hist.history['loss'])
plt.title('Model Loss Progress During Training')
plt.xlabel('Epoch')
plt.ylabel('Training Loss')
plt.legend(['Training Loss'])

plt.plot(epochs_hist.history['accuracy'])
plt.title('Model Accuracy Progress During Training')
plt.xlabel('Epoch')
plt.ylabel('Training Accuracy')
plt.legend(['Training Accuracy'])

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_pred))



# Testing Set Performance



"""# EXCELLENT JOB! YOU SHOULD BE PROUD OF YOUR NEWLY ACQUIRED SKILLS"""